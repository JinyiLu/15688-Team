{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import util\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc(\"figure\", figsize=(8,6))\n",
    "matplotlib.rc(\"axes\", labelsize=16, titlesize=16)\n",
    "matplotlib.rc(\"xtick\", labelsize=14)\n",
    "matplotlib.rc(\"ytick\", labelsize=14)\n",
    "matplotlib.rc(\"legend\", fontsize=14)\n",
    "matplotlib.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18060, 1603) (1603,) (18060,)\n"
     ]
    }
   ],
   "source": [
    "# load the rating matrix\n",
    "M = util.load_sparse_csr(\"imdb_data/rating.npz\").toarray()\n",
    "movie_names = np.load(\"imdb_data/movies.npy\")\n",
    "user_names = np.load(\"imdb_data/users.npy\")\n",
    "\n",
    "print M.shape, movie_names.shape, user_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u' (borkoboardo)' u'surfs_up1976' u'HumanoidOfFlesh' ...,\n",
      " u'Kaya Ozkaracalar' u'LJ27' u'KHayes666']\n"
     ]
    }
   ],
   "source": [
    "print user_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18060, 1603) (1603,) (18060,)\n",
      "504\n",
      "358\n"
     ]
    }
   ],
   "source": [
    "mask = M > 0\n",
    "print mask.shape, mask.sum(axis=0).shape, mask.sum(axis=1).shape\n",
    "print np.sum(mask.sum(axis=0) > 5)\n",
    "print np.sum(mask.sum(axis=1) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18060,) (1603,)\n",
      "Original #rating 25658\n",
      "Remaining #user 358 #movie 504\n",
      "(358, 504)\n",
      "Remaining #rating 4629\n",
      "Sparsity for the rating matrix : 2.57%\n",
      "(358, 504) (358,) (504,)\n"
     ]
    }
   ],
   "source": [
    "new_rating_m, new_user_names, new_movie_names = util.rating_filter(M, user_names, movie_names, (5,5))\n",
    "\n",
    "print new_rating_m.shape, new_user_names.shape, new_movie_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user delete 5\n",
      "(353, 504) 2669 0\n",
      "(353, 504) 1040 0\n",
      "(353, 504) 911 0\n"
     ]
    }
   ],
   "source": [
    "train, valid, test, user_mask = util.split_dataset(new_rating_m)\n",
    "print train.shape, np.sum(train>0), np.sum(np.sum(train, axis=1) == 0)\n",
    "print valid.shape, np.sum(valid>0), np.sum(np.sum(valid, axis=1) == 0)\n",
    "print test.shape, np.sum(test>0), np.sum(np.sum(test, axis=1) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based\n",
    "\n",
    "Turn movie to feature vectors using their plot description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504 (504, 9499) (504, 9499)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "DB_NAME = 'imdb_data/imdb_final.db'\n",
    "\n",
    "def movie_to_vec(dbname, movie_names):\n",
    "    plots = util.load_movie_plot(dbname, movie_names)\n",
    "    all_plots = []\n",
    "    for i in range(len(movie_names)):\n",
    "        p = ''\n",
    "        for plot in plots[movie_names[i]]:\n",
    "            p += plot + ' '\n",
    "        all_plots.append(p)\n",
    "    # tokenizing\n",
    "    count_vect = CountVectorizer()\n",
    "    plot_counts = count_vect.fit_transform(all_plots)\n",
    "    # tfidf\n",
    "    tf_transformer = TfidfTransformer().fit(plot_counts)\n",
    "    plot_tfidf = tf_transformer.transform(plot_counts)    \n",
    "    \n",
    "    return all_plots, plot_counts, plot_tfidf\n",
    "\n",
    "\n",
    "all_plots, plot_counts, plot_tfidf = movie_to_vec(DB_NAME, new_movie_names)\n",
    "print len(all_plots), plot_counts.shape, plot_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content based model, quite like linear regression with L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 63022.000000\n",
      "Loss at iteration 1 26706.397676\n",
      "Loss at iteration 2 15965.656951\n",
      "Loss at iteration 3 11503.460561\n",
      "Loss at iteration 4 9414.371119\n",
      "Loss at iteration 5 8363.269184\n",
      "Loss at iteration 6 7807.130677\n",
      "Loss at iteration 7 7501.454908\n",
      "Loss at iteration 8 7328.275278\n",
      "Loss at iteration 9 7227.689778\n",
      "Loss at iteration 10 7168.037830\n",
      "Loss at iteration 11 7132.029843\n",
      "Loss at iteration 12 7109.961976\n",
      "Loss at iteration 13 7096.259142\n",
      "Loss at iteration 14 7087.653074\n",
      "Loss at iteration 15 7082.193940\n",
      "Loss at iteration 16 7078.700537\n",
      "Loss at iteration 17 7076.447616\n",
      "Loss at iteration 18 7074.984595\n",
      "Loss at iteration 19 7074.028598\n"
     ]
    }
   ],
   "source": [
    "class ContentBased:\n",
    "    def __init__(self, num_user, movie_vec):\n",
    "        self.movie_vec = movie_vec.toarray()\n",
    "        self.theta = np.zeros((num_user, movie_vec.shape[1]))\n",
    "        \n",
    "    def cal_loss(self, rating, lam=0.1):\n",
    "        pred = self.movie_vec.dot(self.theta.T).T\n",
    "        mask = rating > 0\n",
    "        loss = 0.0\n",
    "        \n",
    "        for i in range(rating.shape[0]):\n",
    "            loss += np.sum(np.square(pred[i,:] - rating[i,:]) * mask[i,:])\n",
    "        loss /= 2.0\n",
    "        loss += lam / 2.0 * np.sum(np.square(self.theta))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def cal_grad(self, rating, lam=0.1):\n",
    "        grad = np.zeros(self.theta.shape)\n",
    "        mask = rating > 0\n",
    "        pred = self.movie_vec.dot(self.theta.T).T\n",
    "        diff = pred - rating\n",
    "        for i in range(grad.shape[0]):\n",
    "            grad_user = (mask[i,:] * diff[i,:]).reshape(1,-1) * self.movie_vec.T\n",
    "            grad[i,:] = grad_user.sum(axis=1)\n",
    "        return grad\n",
    "    \n",
    "    def pred(self):\n",
    "        return self.movie_vec.dot(self.theta.T).T\n",
    "        \n",
    "    def train(self, rating, max_ite=100, learning_rate=0.2, lam=0.1):\n",
    "        for i in range(max_ite):\n",
    "            loss = self.cal_loss(rating, lam=lam)\n",
    "            print 'Loss at iteration %d %f' % (i, loss)\n",
    "            grad = self.cal_grad(rating, lam)\n",
    "            self.theta = self.theta - learning_rate * (grad + lam * self.theta)\n",
    "\n",
    "cb = ContentBased(train.shape[0], plot_tfidf)\n",
    "cb.train(train, max_ite=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE at train 3.082829 valid 22.690267\n"
     ]
    }
   ],
   "source": [
    "# compute validation metric\n",
    "def compute_mse(prediction, real):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        prediction (matrix) : prediction of users' ratings\n",
    "        real (matrix) : real user ratings\n",
    "    Output:\n",
    "        mse (double) : mean squared error\n",
    "    \"\"\"\n",
    "    # rule out the empty rating\n",
    "    return np.mean(((real - prediction)**2)[real.nonzero()])\n",
    "\n",
    "\n",
    "mse_train = compute_mse(cb.pred(), train)\n",
    "mse_valid = compute_mse(cb.pred(), valid)\n",
    "\n",
    "print 'MSE at train %f valid %f' % (mse_train, mse_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref\n",
    "* https://www.coursera.org/learn/machine-learning/lecture/uG59z/content-based-recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
