{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import util\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc(\"figure\", figsize=(8,6))\n",
    "matplotlib.rc(\"axes\", labelsize=16, titlesize=16)\n",
    "matplotlib.rc(\"xtick\", labelsize=14)\n",
    "matplotlib.rc(\"ytick\", labelsize=14)\n",
    "matplotlib.rc(\"legend\", fontsize=14)\n",
    "matplotlib.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18060, 1603) (1603,) (18060,)\n"
     ]
    }
   ],
   "source": [
    "# load the rating matrix\n",
    "M = util.load_sparse_csr(\"imdb_data/rating.npz\").toarray()\n",
    "movie_names = np.load(\"imdb_data/movies.npy\")\n",
    "user_names = np.load(\"imdb_data/users.npy\")\n",
    "\n",
    "print M.shape, movie_names.shape, user_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18060,) (1603,)\n",
      "Original #rating 25658\n",
      "Remaining #user 358 #movie 504\n",
      "(358, 504)\n",
      "Remaining #rating 4629\n",
      "Sparsity for the rating matrix : 2.57%\n",
      "(358, 504) (358,) (504,)\n"
     ]
    }
   ],
   "source": [
    "new_rating_m, new_user_names, new_movie_names = util.rating_filter(M, user_names, movie_names, (5,5))\n",
    "\n",
    "print new_rating_m.shape, new_user_names.shape, new_movie_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user delete 5\n",
      "(353, 504) 2669 0\n",
      "(353, 504) 1040 0\n",
      "(353, 504) 911 0\n"
     ]
    }
   ],
   "source": [
    "train, valid, test, user_mask = util.split_dataset(new_rating_m)\n",
    "print train.shape, np.sum(train>0), np.sum(np.sum(train, axis=1) == 0)\n",
    "print valid.shape, np.sum(valid>0), np.sum(np.sum(valid, axis=1) == 0)\n",
    "print test.shape, np.sum(test>0), np.sum(np.sum(test, axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'imdb_data/imdb_final.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute validation metric\n",
    "def compute_mse(prediction, real):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        prediction (matrix) : prediction of users' ratings\n",
    "        real (matrix) : real user ratings\n",
    "    Output:\n",
    "        mse (double) : mean squared error\n",
    "    \"\"\"\n",
    "    # rule out the empty rating\n",
    "    return np.mean(((real - prediction)**2)[real.nonzero()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.x Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based filtering methods rely on the description of the item and a profile of the user's preference to make the recommendation. In a content-based recommender system, we firstly use the description of the item to create a vector representation of all the items. Then we need to build a user profile for every users which will indicate what kinds of items this user likes. So the main idea of content-based filtering is that users will like the items that are similar to those them liked in the past (based on their profile). So there are two main problem in order to use the content-based filtering.\n",
    "\n",
    "* How to generate an abstract representation of items?\n",
    "    * In our project, items are movies which contains various aspects, including plots, genres, credits, etc.\n",
    "* How to build a user profile for every users?\n",
    "    * Generally, a user profile can be built based on user's previous preference and/or user's history interaction with the system. However, in our project, we have no access to the user's interaction with IMDb. So we will only use their history preference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.x.1 How to Learn the Content-Based Filtering Model\n",
    "\n",
    "Let \n",
    "* $\\theta^{(i)}$ be the profile representation of the i-th user (parameters of the model that we need to learn),\n",
    "* $m^{(j)}$ be the vector representation of the j-th movie (created from other sources),\n",
    "* $y^{(i,j)}$ be the rating for the j-th movie given by i-th user,\n",
    "* $r^{(i,j)}$ be the mask for representing whether i-th user rates j-th motive. $r^{(i,j)} = 1$ means i-th user gives a rating to j-th movie. $r^{(i,j)} = 0$ means otherwiese,\n",
    "* $N$ be the number of users,\n",
    "* $M$ be the number of movies,\n",
    "* $K$ be the dimension of the representation of user profile and movie.\n",
    "\n",
    "So i-th user's potentional perference about j-th movie can be calculate by in this model:\n",
    "\n",
    "\\begin{align}\n",
    "{\\theta^{(i)}}^T m^{(j)} \\notag\n",
    "\\end{align}\n",
    "\n",
    "In order to learn the best representation of $\\theta^{(i)}$ for i-th user, we need to optimize the following loss function based on the training set:\n",
    "\n",
    "\\begin{align}\n",
    "loss^{(i)} = \\frac{1}{2} \\sum_{j:r^{(i,j)}=1} ({\\theta^{(i)}}^T m^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{K} (\\theta_k^{(i)}) ^2 \\notag\n",
    "\\end{align}\n",
    "\n",
    "In this loss function, we consider the square loss of all rated movies with l2 penalty to avoid overfitting.\n",
    "\n",
    "Since we have more than one user to consider, the total loss is sum over the loss of all users.\n",
    "\n",
    "\\begin{align}\n",
    "loss = \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j:r^{(i,j)}=1} ({\\theta^{(i)}}^T m^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (\\theta_k^{(i)}) ^2 \\notag\n",
    "\\end{align} \n",
    "\n",
    "So this is our final optimization objective. It's quite similar to the one in linear regression. And in order to minimize such loss we can use gradient descent.\n",
    "\n",
    "Gradient descent update is\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_k^{(i)} := \\theta_k^{(i)} - \\alpha (\\sum_{j:r^{(i,j)}=1} ({\\theta^{(i)}}^T m^{(j)} - y^{(i,j)}) m_k^{(j)} + \\lambda \\theta_k^{(i)} \\notag\n",
    "\\end{align}\n",
    "\n",
    "In which $\\lambda$ is the parameters for control the l2 penalty and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContentBased:\n",
    "    def __init__(self, num_user, movie_vec):\n",
    "        if type(movie_vec) != np.ndarray:\n",
    "            self.movie_vec = movie_vec.toarray()\n",
    "        else:\n",
    "            self.movie_vec = movie_vec\n",
    "        self.theta = np.zeros((num_user, movie_vec.shape[1]))\n",
    "        \n",
    "    def cal_loss(self, rating, lam=0.1):\n",
    "        pred = self.movie_vec.dot(self.theta.T).T\n",
    "        mask = rating > 0\n",
    "        loss = np.sum(np.square(pred - rating) * mask)      \n",
    "        loss /= 2.0\n",
    "        loss_l2 = loss + lam / 2.0 * np.sum(np.square(self.theta))\n",
    "        \n",
    "        return loss, loss_l2\n",
    "    \n",
    "    def cal_grad(self, rating, lam=0.1):\n",
    "        grad = np.zeros(self.theta.shape)\n",
    "        mask = rating > 0\n",
    "        pred = self.movie_vec.dot(self.theta.T).T\n",
    "        diff = pred - rating\n",
    "        \n",
    "        grad = (mask * diff).dot(self.movie_vec)\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def pred(self):\n",
    "        return self.movie_vec.dot(self.theta.T).T\n",
    "        \n",
    "    def train(self, rating, valid=None, max_ite=100, learning_rate=0.2, lam=0.1):\n",
    "        for i in range(max_ite):\n",
    "            loss, loss_l2 = self.cal_loss(rating, lam=lam)\n",
    "            if i%10 == 0:\n",
    "                if valid is None:\n",
    "                    print 'Iteration %d train loss: %f' % (i, loss)\n",
    "                else:\n",
    "                    v_loss, v_loss_l2 = self.cal_loss(valid, lam=lam)\n",
    "                    print 'Iteration %d train loss: %f valid loss: %f' % (i, loss, v_loss)\n",
    "            grad = self.cal_grad(rating, lam)\n",
    "            self.theta = self.theta - learning_rate * (grad + lam * self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.x.2 Use Genres to Represent Movies\n",
    "\n",
    "So there are many ways to generate the vector representation of the movies. Firstly, let's have a try of genres information. The following code will represent each movie according which genre it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movie_to_vec_genre(dbname, movie_names):\n",
    "    movie_genres = {}\n",
    "    all_genres = set()\n",
    "    \n",
    "    conn = sqlite3.connect(dbname)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    for r in c.execute('''\n",
    "        SELECT imdb_id, genres\n",
    "        FROM movie\n",
    "        '''):\n",
    "        if r[0] in movie_names:\n",
    "            gs = ast.literal_eval(r[1])\n",
    "            if gs is None:\n",
    "                continue\n",
    "            all_genres |= set(gs)\n",
    "            movie_genres[r[0]] = gs\n",
    "    print len(all_genres), len(movie_genres)\n",
    "    \n",
    "    f_genres = list(all_genres)\n",
    "    movie_vec = np.zeros((len(movie_names), len(f_genres)))\n",
    "    for i in range(len(movie_names)):\n",
    "        for g in movie_genres[movie_names[i]]:\n",
    "            movie_vec[i,f_genres.index(g)] = 1.0\n",
    "    return movie_vec, f_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 504\n",
      "(504, 24)\n",
      "[u'Mystery', u'Short', u'Sci-Fi', u'Crime', u'Drama', u'Animation', u'Music', u'Action', u'Comedy', u'Documentary', u'War', u'History', u'Romance', u'Family', u'Horror', u'Thriller', u'Film-Noir', u'Musical', u'Fantasy', u'Adventure', u'News', u'Sport', u'Biography', u'Western']\n",
      "Example genres for movie tt0456999\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "movie_vec_genres, f_genres = movie_to_vec_genre(DB_NAME, new_movie_names)\n",
    "print movie_vec_genres.shape\n",
    "print f_genres\n",
    "\n",
    "print 'Example genres for movie %s\\n%s' % (new_movie_names[0], movie_vec_genres[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 24 genres in total for all 504 movies. Let's use this metrix as the representation of movies for our Content-Based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 train loss: 63022.000000 valid loss: 26524.500000\n",
      "Iteration 10 train loss: 16612.620059 valid loss: 12012.547503\n",
      "Iteration 20 train loss: 9326.644622 valid loss: 9502.152275\n",
      "Iteration 30 train loss: 6656.762464 valid loss: 8633.983846\n",
      "Iteration 40 train loss: 5330.488536 valid loss: 8279.680470\n",
      "MSE at train 3.408181 valid 15.638814\n"
     ]
    }
   ],
   "source": [
    "cb = ContentBased(train.shape[0], movie_vec_genres)\n",
    "cb.train(train, valid=valid, max_ite=50, learning_rate=0.01, lam=0.1)\n",
    "\n",
    "mse_train = compute_mse(cb.pred(), train)\n",
    "mse_valid = compute_mse(cb.pred(), valid)\n",
    "\n",
    "print 'MSE at train %f valid %f' % (mse_train, mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, we can achieve about 15.639 mean square error when use this model to predict user's perference about movies and give recommendation. It means on average difference between the ratings given by this model and the actual ratings is abount 3.95. (rating are in range of 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.x.3 Use Plots to Represent Movies\n",
    "\n",
    "There are only 24 genres, how about tring some richer information? Movie plots might be a good choice since usually it contains large chunk of text. We choice to use the tf-idf algorithm to try plots into vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movie_to_vec_plot(dbname, movie_names):\n",
    "    plots = util.load_movie_plot(dbname, movie_names)\n",
    "    all_plots = []\n",
    "    for i in range(len(movie_names)):\n",
    "        p = ''\n",
    "        for plot in plots[movie_names[i]]:\n",
    "            p += plot + ' '\n",
    "        all_plots.append(p)\n",
    "    # tokenizing\n",
    "    count_vect = CountVectorizer()\n",
    "    plot_counts = count_vect.fit_transform(all_plots)\n",
    "    # tfidf\n",
    "    tf_transformer = TfidfTransformer().fit(plot_counts)\n",
    "    plot_tfidf = tf_transformer.transform(plot_counts)    \n",
    "    \n",
    "    return all_plots, plot_counts, plot_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504 (504, 9499) (504, 9499)\n",
      "Example plots for movie tt0456999\n",
      "The film deals with a boisterous undercover female cop who gets sent to a high school in order to get close to a criminal in hideout by befriending his teenage daughter. The general set up and the fighting antics of the female cop play close resemblance to Stephen Chow's classic Fight Back to School. \n"
     ]
    }
   ],
   "source": [
    "all_plots, plot_counts, plot_tfidf = movie_to_vec_plot(DB_NAME, new_movie_names)\n",
    "print len(all_plots), plot_counts.shape, plot_tfidf.shape\n",
    "\n",
    "print 'Example plots for movie %s\\n%s' % (new_movie_names[0], all_plots[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the vocabulary size of the plots we have it quite large (9499 distinct words). If we directly use such representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 train loss: 63022.000000 valid loss: 26524.500000\n",
      "Iteration 10 train loss: 4401.264583 valid loss: 11945.853378\n",
      "Iteration 20 train loss: 4114.034755 valid loss: 11798.938670\n",
      "Iteration 30 train loss: 4096.210943 valid loss: 11794.504433\n",
      "Iteration 40 train loss: 4094.333932 valid loss: 11794.474821\n",
      "MSE at train 3.067886 valid 22.681732\n"
     ]
    }
   ],
   "source": [
    "cb = ContentBased(train.shape[0], plot_tfidf)\n",
    "cb.train(train, valid=valid, max_ite=50, lam=0.1)\n",
    "\n",
    "mse_train = compute_mse(cb.pred(), train)\n",
    "mse_valid = compute_mse(cb.pred(), valid)\n",
    "\n",
    "print 'MSE at train %f valid %f' % (mse_train, mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using all 9499 features will easily lead to overfit. We can achieve a very good performance on the training set. However on the validation set, our performance is quite bad. It's because since the dimension of user profile and movie representation are the same. We have $N*K = 358 * 9499 = 3.4M$ parameters to learn but we only have about 2 thousand training examples. So it's necessary to do feature selection in this case to speed up the training process and avoid overfitting.\n",
    "\n",
    "In the following we choose to use coefficients of a linear model to select the features. We fit a linear support vector regression model to predict the average score of the movies with the tf-idf feature from movie plots. And then only keep features with high coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_selection(X, y, th='7*mean'):\n",
    "    lsvr = LinearSVR(C=0.03).fit(X, y)\n",
    "    model = SelectFromModel(lsvr, prefit=True, threshold='7*mean')\n",
    "    X_new = model.transform(X)\n",
    "    print X_new.shape\n",
    "    print np.sum(lsvr.coef_ > 0)\n",
    "    \n",
    "    lsvr.fit(X_new, y)\n",
    "    y_pred = lsvr.predict(X_new)\n",
    "    print np.mean(np.square(y_pred - y))\n",
    "    print model\n",
    "    mask = model.get_support(indices=False)\n",
    "    print np.sum(mask)\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 71)\n",
      "6462\n",
      "10.4295463304\n",
      "SelectFromModel(estimator=LinearSVR(C=0.03, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     random_state=None, tol=0.0001, verbose=0),\n",
      "        prefit=True, threshold='7*mean')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y = train.sum(axis=0) / ((train>0).sum(axis=0)+1e-4)\n",
    "movie_vec_plot_sel = feature_selection(plot_tfidf, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, finally we only keep 71 features out of 9499 features. If we run the Content-Based Filtering again, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 train loss: 63022.000000 valid loss: 26524.500000\n",
      "Iteration 10 train loss: 13345.818953 valid loss: 8593.730959\n",
      "Iteration 20 train loss: 10236.443355 valid loss: 7225.748653\n",
      "Iteration 30 train loss: 9213.591353 valid loss: 6852.688442\n",
      "Iteration 40 train loss: 8710.132396 valid loss: 6716.331497\n",
      "MSE at train 6.306739 valid 12.803903\n"
     ]
    }
   ],
   "source": [
    "cb = ContentBased(train.shape[0], movie_vec_plot_sel)\n",
    "cb.train(train, valid=valid, max_ite=50, lam=0.1)\n",
    "\n",
    "mse_train = compute_mse(cb.pred(), train)\n",
    "mse_valid = compute_mse(cb.pred(), valid)\n",
    "\n",
    "print 'MSE at train %f valid %f' % (mse_train, mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is greatly improved even with much fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reference\n",
    "* https://www.coursera.org/learn/machine-learning/lecture/uG59z/content-based-recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
